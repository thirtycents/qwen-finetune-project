# Qwen3-0.6B Function Calling 项目知识点大纲

> **项目定位**：2026年 AI 行业标配技能——基于 Qwen3-0.6B 的 AI Agent 工具调用全链路工程化实践。
> **核心目标**：通过 SFT + GRPO 强化学习，将 0.6B 小模型训练为具备生产级 Function Calling 能力的助手。
> **文档说明**：本大纲涵盖模型微调、推理优化、部署运维、数据工程、评估体系、强化学习、AI Agent 及工程实践八大模块，旨在为面试准备和技术复盘提供全面参考。

---

## 0. 如何使用本大纲 (How to Use)

*   **面试前夕**：重点复习各模块的 **面试 Q&A**，确保能够流利回答核心原理和工程细节。
*   **技术复盘**：对照 **关键参数** 和 **相关代码**，深入理解项目中的每一行配置和逻辑。
*   **深度学习**：通过 **核心原理** 部分的公式和概念，建立起大模型微调与部署的知识体系。

---

## 1. 模型微调 (Model Fine-tuning)

### 核心原理
*   **LoRA (Low-Rank Adaptation)**: 
    *   **原理**：在冻结原始模型权重 $W \in \mathbb{R}^{d \times k}$ 的基础上，引入两个低秩矩阵 $A \in \mathbb{R}^{r \times k}$ 和 $B \in \mathbb{R}^{d \times r}$，其中 $r \ll \min(d, k)$。
    *   **公式**：$W_{new} = W + \Delta W = W + B \times A$。在推理时，可以将 $BA$ 合并回 $W$，实现零额外推理延迟。
    *   **优势**：显存占用极低（仅需存储 $A$ 和 $B$ 的梯度和优化器状态），训练速度快，原模型权重不变，可实现多任务适配器的快速切换。
*   **SFT (Supervised Fine-tuning)**:
    *   **原理**：使用高质量的“指令-回复”对进行监督学习。在本项目中，重点是让模型学会从自然语言中提取参数并构建符合 JSON Schema 的输出。
    *   **格式**：采用 ShareGPT 格式，通过 `system` 角色注入工具定义，`human` 角色提出问题，`gpt` 角色输出工具调用。
*   **量化 (Quantization)**:
    *   **GGUF**: 一种专为大模型推理设计的二进制文件格式，支持高效的权重加载和量化。
    *   **Q4_K_M vs Q8_0**: Q4_K_M（4位量化）在保持较高精度的同时极大压缩了模型体积（约 350MB），适合移动端；Q8_0（8位量化）精度损失极小，适合桌面端。

### 关键参数配置 (configs/qwen3_lora_sft.yaml)
| 参数 | 推荐值 | 说明 |
| :--- | :--- | :--- |
| `model_name_or_path` | Qwen/Qwen3-0.6B | 基座模型路径 |
| `template` | qwen3 | 对话模板，必须与模型匹配 |
| `finetuning_type` | lora | 微调方法 |
| `lora_rank` | 32 | 低秩矩阵的秩。对于 0.6B 模型，32 提供了足够的表达能力。 |
| `lora_alpha` | 64 | 缩放因子。经验法则：`alpha = 2 * rank`，有助于稳定训练。 |
| `lora_target` | all | 对模型所有线性层应用 LoRA，效果优于仅针对注意力层。 |
| `num_train_epochs` | 3 | 训练轮数。对于 60k 数据，3 轮通常能达到收敛且不过拟合。 |
| `learning_rate` | 2e-4 | LoRA SFT 的标准学习率，通常高于全参数微调。 |
| `bf16` | true | BFloat16 精度。相比 FP16，BF16 动态范围更大，能有效防止梯度溢出。 |
| `gradient_checkpointing` | true | 梯度检查点，用计算换显存，适合 12GB 显存环境。 |

### 面试 Q&A
*   **Q1: 为什么选择 Qwen3-0.6B 作为模型基础？**
    *   **A**: Qwen3-0.6B 是通义千问团队发布的轻量化模型，具备极高的性价比。它体积小（量化后可运行在手机端）、推理延迟低，且在 Function Calling 这种格式化任务中，通过针对性微调可以展现出不亚于 7B 甚至更大模型的准确率。
*   **Q2: 为什么使用 LoRA 而不是全参数微调？**
    *   **A**: LoRA 极大降低了显存门槛（12GB 显存即可训练），且训练产物（Adapter）体积小，便于分发和多任务切换。
*   **Q3: 为什么 LoRA 训练时要设置 `lora_target: all`？**
    *   **A**: 研究表明，将 LoRA 应用于 Transformer 的所有线性层（包括 MLP 层）比仅应用于注意力层（Q, V）能获得更好的微调效果，尤其是在需要模型学习新知识或复杂格式的任务中。
*   **Q4: 什么是灾难性遗忘（Catastrophic Forgetting）？如何避免？**
    *   **A**: 灾难性遗忘是指模型在学习新任务时，由于权重的大幅更新导致其在旧任务上的表现剧烈下降。在本项目中，通过使用 LoRA（仅更新少量参数）和设置较小的学习率，可以有效缓解这一问题。

---

## 2. 推理优化 (Inference Optimization)

### 核心原理
*   **PagedAttention**: 
    *   **原理**：受操作系统分页内存管理启发，将 KV Cache 划分为固定大小的块（Blocks），并允许它们在物理内存中非连续存储。
    *   **优势**：消除了 KV Cache 的内部和外部碎片，使得显存利用率接近 100%，从而支持更大的 Batch Size 和更高的并发。
*   **Continuous Batching (持续批处理)**:
    *   **原理**：传统的动态批处理（Dynamic Batching）需要等待整个 batch 的所有序列都生成完毕。持续批处理允许在请求级别进行迭代，一旦某个请求生成结束，立即释放其占用的资源并插入新请求。
    *   **对比**：相比传统方式，吞吐量可提升 2-4 倍。
*   **Prefix Caching (前缀缓存)**:
    *   **原理**：在 Function Calling 场景中，系统提示词（System Prompt）和工具定义（Tools Schema）通常是固定且冗长的。Prefix Caching 缓存这些公共前缀的 KV Cache，避免重复计算。

### 关键指标与定义
*   **TTFT (Time-To-First-Token)**: 首 token 延迟。对于 Agent 交互，TTFT 直接影响用户的“即时感”。
*   **Throughput (吞吐量)**: 每秒处理的 token 总数（Input + Output）。
*   **TBT (Time-Between-Tokens)**: 相邻两个 token 生成的平均间隔。
*   **P99 Latency**: 99% 的请求都在此延迟内完成，衡量系统的长尾稳定性。

### 面试 Q&A
*   **Q1: vLLM 为什么能实现高吞吐？**
    *   **A**: 核心在于 PagedAttention 解决了显存碎片问题，使得单卡能承载更多并发请求；配合 Continuous Batching 减少了 GPU 的空转时间；以及高效的 CUDA Kernel 优化。
*   **Q2: 在 Function Calling 任务中，Prefix Caching 为什么特别有效？**
    *   **A**: 因为每个请求都会携带相同的工具定义（Tools Schema），这部分内容往往占据了 Prompt 的很大比例。缓存这部分 KV Cache 可以显著降低计算量，缩短 TTFT。
*   **Q3: 什么是 TTFT 和 TPOT？**
    *   **A**: TTFT 是 Time To First Token（首 token 延迟）；TPOT 是 Time Per Output Token（每个输出 token 的平均延迟）。在实时对话中，TTFT 决定了响应的快慢，TPOT 决定了打字机效果的流畅度。

---

## 3. 部署与运维 (Deployment & MLOps)

### 核心组件
*   **Docker 容器化**: 
    *   `Dockerfile.inference`: 采用多阶段构建，集成 vLLM 运行时环境，优化镜像体积。
*   **Kubernetes (K8s) 编排**:
    *   **Deployment**: 定义副本数、滚动更新策略（RollingUpdate）。
    *   **Service**: 通过 ClusterIP 实现集群内负载均衡。
    *   **Health Checks**: 
        *   `LivenessProbe`: 检查容器是否存活，失败则重启。
        *   `ReadinessProbe`: 检查模型是否加载完成，准备好接收流量。
*   **监控体系 (Prometheus + Grafana)**:
    *   **指标采集**：监控 vLLM 的 `/metrics` 端点。
    *   **核心面板**：TTFT 趋势图、KV Cache 利用率、请求排队情况、GPU 显存占用。

### 关键配置 (deploy/k8s/deployment.yaml)
*   `replicas`: 1 (单 GPU 环境)
*   `nvidia.com/gpu`: 1 (请求 1 块 GPU)
*   `max_num_seqs`: 最大并发序列数。
*   `max_num_batched_tokens`: 每个迭代处理的最大 token 数。
*   `terminationGracePeriodSeconds`: 优雅终止时间，确保 Pod 退出前处理完当前请求。

### 面试 Q&A
*   **Q1: 如何处理模型加载时间过长导致的 K8s 启动失败？**
    *   **A**: 使用 `startupProbe`（启动探针）。它专门用于处理慢启动容器，在 `startupProbe` 成功之前，`liveness` 和 `readiness` 探针都不会生效，从而避免 Pod 在加载模型时被误杀。
*   **Q2: 生产环境中如何监控推理服务的性能瓶颈？**
    *   **A**: 重点关注 KV Cache 利用率和请求队列长度。如果 KV Cache 长期处于 90% 以上且队列中有大量等待请求，说明显存已达瓶颈，需要扩容或优化 `max_num_seqs`。
*   **Q3: 什么是滚动更新（Rolling Update）？**
    *   **A**: 滚动更新是 K8s 默认的更新策略，它通过逐个替换旧版本的 Pod 来实现不停机发布。在 `deployment.yaml` 中可以通过 `maxSurge` 和 `maxUnavailable` 来精细控制更新过程。

---

## 4. 数据工程 (Data Engineering)

### 核心流程
*   **数据集选型**: `Salesforce/xlam-function-calling-60k`。
    *   **特点**：由 APIGen 生成，包含 60k 高质量样本，覆盖单函数、多函数及并行调用场景。
*   **三层校验机制 (3-Layer Validation)**:
    1.  **格式校验 (Format)**: 确保输出符合 JSON 语法。
    2.  **执行校验 (Execution)**: 验证参数是否满足函数定义的类型约束（如 string, integer）。
    3.  **语义校验 (Semantic)**: 验证参数值是否逻辑合理（如日期格式、城市名称）。
*   **数据预处理**:
    *   将原始数据转换为 LLaMA-Factory 的 ShareGPT 格式。
    *   **90/10 Split**: 严格划分训练集和验证集，确保评估的公正性。

### 数据格式示例 (ShareGPT)
```json
{
  "conversations": [
    { "from": "human", "value": "北京今天天气如何？" },
    { "from": "function_call", "value": "{\"name\": \"get_weather\", \"arguments\": {\"city\": \"北京\"}}" },
    { "from": "observation", "value": "{\"status\": \"success\", \"temperature\": \"25C\"}" },
    { "from": "gpt", "value": "北京今天晴，气温 25 摄氏度。" }
  ],
  "system": "你是一个具备工具调用能力的助手...",
  "tools": "[{\"name\": \"get_weather\", ...}]"
}
```

### 面试 Q&A
*   **Q1: 为什么在数据准备阶段要添加 `observation` 角色？**
    *   **A**: `observation` 模拟了工具执行后的返回结果。通过这种方式，模型不仅学会了如何“发起”调用，还学会了如何根据“执行结果”生成最终的自然语言回复，实现了完整的 Agent 闭环。
*   **Q2: 如何处理数据中的长尾分布问题？**
    *   **A**: 在 Function Calling 中，某些函数可能出现频率极高，而某些则极低。可以通过过采样（Oversampling）稀有函数样本或使用数据增强技术来平衡数据集。

---

## 5. 评估体系 (Evaluation)

### 离线质量指标 (Offline Metrics)
1.  **解析成功率 (Parse Rate)**:
    *   **定义**: $N_{parsed} / N_{total}$。
    *   **逻辑**: 检查输出是否为合法 JSON 且包含 `name` 字段。
2.  **Schema 命中率 (Schema Hit)**:
    *   **逻辑**: 检查函数名是否在可用列表中，且所有 `required` 参数是否都已提供。
3.  **参数 F1 分数 (Parameter F1)**:
    *   **公式**: $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$。
    *   **逻辑**: 将参数键值对视为集合，计算预测与真值的交集。
4.  **可执行率 (Exec Rate)**:
    *   **逻辑**: 检查参数类型是否符合 Schema 定义（如 `integer` 必须是数字）。
5.  **函数名准确率 (Func Accuracy)**:
    *   **逻辑**: 预测函数名与真值完全一致的比例。
6.  **语义一致性 (Semantic Match)**:
    *   **逻辑**: 参数值（忽略大小写和空格）与真值的匹配程度。

### 面试 Q&A
*   **Q1: 如何区分“格式错误”和“逻辑错误”？**
    *   **A**: `Parse Rate` 衡量格式错误（如少个括号）；`Schema Hit` 和 `Parameter F1` 衡量逻辑错误（如选错函数或填错参数）。
*   **Q2: 为什么参数评估要用 F1 而不是 Accuracy？**
    *   **A**: 因为一个函数调用可能包含多个参数。Accuracy 要求全对才给分，太严苛；F1 可以体现模型“对了几分之几”，评估更精细。
*   **Q3: 什么是 AST 评测？**
    *   **A**: AST（Abstract Syntax Tree，抽象语法树）评测是指将模型输出解析为语法树，然后与标准答案的语法树进行对比。这比简单的字符串匹配更准确，能处理 JSON 字段顺序不同等情况。

---

## 6. 强化学习 (Reinforcement Learning)

### GRPO 算法 (Group Relative Policy Optimization)
*   **原理**：对每个 Prompt 采样 $G$ 个回答，计算这 $G$ 个回答的平均奖励 $\bar{r}$，第 $i$ 个回答的优势值为 $A_i = (r_i - \bar{r}) / \sigma$。
*   **优势**：
    *   **无 Critic 模型**：显著降低显存占用（12GB 显存即可运行）。
    *   **规则驱动**：在 Function Calling 任务中，奖励可以通过代码逻辑（JSON 解析、Schema 校验）自动计算，无需昂贵的人工偏好标注。
*   **对比表**：
    | 特性 | PPO | DPO | GRPO |
    | :--- | :--- | :--- | :--- |
    | **奖励来源** | 奖励模型 (RM) | 偏好对 (Chosen/Rejected) | 组内相对得分 (规则/RM) |
    | **显存开销** | 极高 (需加载 4 个模型) | 中 (需加载 2 个模型) | 低 (仅需加载 1 个模型+参考模型) |
    | **稳定性** | 较差，调参复杂 | 较好 | 极佳 |

### 奖励函数设计 (Reward Function)
采用加权求和方式（各占 0.25 权重）：
1.  **JSON Parse Reward**: 成功解析得 1.0，失败得 0。
2.  **Schema Hit Reward**: 函数名在可用列表中且必填项齐全得 1.0。
3.  **Execution Reward**: 参数类型（int, str 等）完全匹配得 1.0。
4.  **Semantic Reward**: 参数值与标准答案的 F1 分数。

### 面试 Q&A
*   **Q1: 为什么 GRPO 适合小显存环境？**
    *   **A**: 因为它取消了 PPO 中的 Value Network（Critic 模型），减少了模型参数的加载量。同时，通过组内采样计算优势值，避免了维护复杂的奖励模型。
*   **Q2: 强化学习阶段如何防止模型“退化”成只会输出 JSON 而失去对话能力？**
    *   **A**: 引入 KL 散度约束（KL Divergence Penalty）。在损失函数中加入当前策略与参考模型（SFT 后的模型）之间的 KL 散度，限制模型偏离原始分布太远。
*   **Q3: 什么是奖励稀疏问题？如何解决？**
    *   **A**: 奖励稀疏是指模型很难通过随机采样获得正向奖励。在本项目中，我们通过先进行 SFT（监督微调）让模型具备基本的 JSON 输出能力，从而在 GRPO 阶段更容易获得正向奖励。

---

## 7. AI Agent 与工具调用

### 核心概念
*   **AI Agent**: 一个能够感知环境、进行推理、做出决策并执行动作的智能系统。
*   **Function Calling**: Agent 的“手”，是连接 LLM 与外部世界的桥梁。
*   **MCP (Model Context Protocol)**: 2026 年行业标准，旨在统一 AI 模型与外部工具/上下文的交互规范。

### Agent 架构位置
1.  **Perception (感知)**: 接收用户输入。
2.  **Planning (规划)**: 模型判断是否需要调用工具，以及调用的顺序。
3.  **Execution (执行)**: **Function Calling 发生在此处**，模型输出 JSON。
4.  **Observation (观察)**: 获取工具执行结果。
5.  **Reflection (反思/总结)**: 模型根据结果生成最终回答。

### 面试 Q&A
*   **Q1: Function Calling 和插件（Plugins）有什么区别？**
    *   **A**: 插件通常是端到端的黑盒；Function Calling 则是将“决策”和“执行”分离。模型只负责生成“调用指令”，执行权在开发者手中，这提供了更高的安全性和灵活性。
*   **Q2: 如何处理多步工具调用（Multi-step Tool Use）？**
    *   **A**: 通过多轮对话实现。模型先输出第一个调用，系统返回结果后，模型再根据结果决定是否发起第二个调用。
*   **Q3: 什么是 ReAct 模式？**
    *   **A**: ReAct (Reason + Act) 是一种 Agent 提示策略，要求模型在执行动作之前先写下“思考（Thought）”过程。这有助于提高复杂任务的推理准确性。

---

## 8. 工程实践 (Engineering Practices)

### CI/CD 与自动化
*   **GitHub Actions**: 
    *   **Linting**: 使用 `ruff` 或 `flake8` 保证代码风格统一。
    *   **Testing**: 使用 `pytest` 运行 `tests/test_metrics.py`，确保评测逻辑无误。
    *   **Building**: 自动构建推理镜像并推送到私有仓库。

### 实验可复现性
*   **Version Locking**: 在 `requirements.txt` 中锁定 `vllm`, `llamafactory`, `torch` 等核心库的版本。
*   **Seed Management**: 训练、数据切分、采样过程统一使用 `seed=42`。

### 常见坑点与方案
*   **显存溢出 (OOM)**: 
    *   方案：开启 `gradient_checkpointing`；使用 `bf16`；减小 `per_device_train_batch_size` 并增加 `gradient_accumulation_steps`。
*   **模型不输出 JSON**: 
    *   原因：SFT 轮数不足或 System Prompt 引导不够。
    *   方案：增加 SFT 轮数，或在 Prompt 中加入 Few-shot 示例。
*   **模板不匹配**: 
    *   必须确保训练与推理使用完全一致的 `template: qwen3`。

### 面试 Q&A
*   **Q1: 2026 年，为什么 0.6B 小模型在工业界仍有巨大价值？**
    *   **A**: (1) **隐私与安全**：端侧运行，数据不出本地；(2) **极低成本**：无需昂贵的 A100/H100 集群；(3) **响应速度**：毫秒级响应，适合实时交互场景。
*   **Q2: 项目中如何处理模型输出的幻觉问题？**
    *   **A**: 通过 GRPO 强化学习，对产生幻觉（调用不存在的函数或参数）的样本给予负奖励，强制模型在严格的 Schema 约束下生成内容。

---

## 9. 常见面试场景模拟 (Interview Scenarios)

### 场景 1：模型在复杂场景下输出格式错误
*   **面试官**：如果模型在处理包含多个嵌套参数的复杂函数时，经常出现 JSON 格式错误，你会如何优化？
*   **回答**：
    1.  **数据层面**：增加复杂嵌套参数的 Few-shot 示例，或者在训练集中针对性地增加此类样本。
    2.  **训练层面**：在 GRPO 阶段调高 `json_parse_reward` 的权重，强制模型优先保证格式正确。
    3.  **工程层面**：在推理端引入结构化输出约束（如 vLLM 的 Guided Decoding），通过 Logits Processor 强制模型只输出符合语法的 token。

### 场景 2：模型调用了错误的函数
*   **面试官**：当用户意图模糊时，模型可能会调用错误的工具。如何提升模型在模糊意图下的判断力？
*   **回答**：
    1.  **对齐训练**：在训练数据中加入“拒绝调用”的负样本，即当用户意图不明确或没有合适工具时，模型应回复“我需要更多信息”而不是强行调用。
    2.  **奖励设计**：在 GRPO 中引入“负奖励”，如果模型在不该调用工具时发起了调用，给予较大的惩罚。
    3.  **Agent 架构**：在调用前增加一个“意图确认”步骤，或者让模型先输出“思考过程（Thought）”，通过思维链（CoT）提升决策准确性。

---

## 10. 技术难点与攻克 (Technical Challenges)

### 难点 1：小模型在长上下文下的性能退化
*   **挑战**：0.6B 模型在处理包含 20+ 个工具定义的 Prompt 时，容易丢失关键信息。
*   **对策**：
    1.  **Prompt 压缩**：仅将与当前意图最相关的 Top-K 工具放入 Prompt。
    2.  **微调增强**：在 SFT 阶段加入大量长上下文样本，提升模型的长程注意力。

### 难点 2：强化学习训练的不稳定性
*   **挑战**：GRPO 训练初期 Loss 剧烈震荡，模型输出变得混乱。
*   **对策**：
    1.  **Warmup**：设置较长的学习率预热期。
    2.  **奖励平滑**：对奖励值进行裁剪（Clipping），防止极端奖励值导致梯度爆炸。

---

## 11. 项目亮点总结 (Project Highlights)

1.  **全链路覆盖**：从原始数据处理到 K8s 生产部署，展示了完整的 MLOps 闭环能力。
2.  **前沿算法应用**：采用了 DeepSeek-R1 同款的 GRPO 算法，体现了对最新技术的追踪和落地能力。
3.  **极致性能优化**：通过 vLLM PagedAttention 和量化技术，在 12GB 显存上实现了高并发推理。
4.  **可量化成果**：所有优化均有明确的离线/在线指标支撑，解析率从 Base 的 ~5% 提升至 95%+。

---

## 12. 硬件选型与成本分析 (Hardware & Cost)

### 训练硬件
*   **最低配置**: NVIDIA RTX 3060 12GB (单卡)。
*   **推荐配置**: NVIDIA RTX 4090 24GB 或 A100 40GB。
*   **显存优化**: 开启 `gradient_checkpointing` 后，0.6B 模型 LoRA 训练仅需约 8GB 显存。

### 推理成本
*   **云端部署**: 1x T4 GPU (16GB) 可支持并发 16，月成本约 $100-$200。
*   **端侧部署**: 0 成本，仅需手机/笔记本具备 4GB 以上可用内存。

---

## 13. 团队协作与版本管理 (Collaboration)

### Git 工作流
*   **Feature Branch**: 每个功能（如 GRPO 奖励函数开发）在独立分支进行。
*   **Code Review**: 重点检查奖励函数的逻辑严密性和评测指标的准确性。

### 模型版本管理
*   **Hugging Face Hub**: 使用私有仓库存储不同阶段的模型权重（SFT-v1, GRPO-v1）。
*   **Model Card**: 详细记录每个版本的训练参数、数据集版本和评测分数。

---

## 14. 常见面试题深度解析 (Deep Dive)

### Q: 什么是 KV Cache？为什么要优化它？
*   **解析**：在 Transformer 推理中，每个 token 的生成都需要之前所有 token 的 Key 和 Value 向量。KV Cache 将这些向量缓存起来，避免重复计算。优化 KV Cache（如 PagedAttention）是为了减少显存占用和碎片，从而支持更高的并发。

### Q: 什么是 BFloat16？它和 Float16 有什么区别？
*   **解析**：BFloat16 (Brain Floating Point) 是一种 16 位浮点数格式，它具有与 Float32 相同的指数位数（8位），但尾数较短。这使得它在处理大数值范围时比 Float16 更稳定，不容易出现梯度爆炸或消失，非常适合大模型训练。

---

## 15. 项目复盘与自我评价 (Retrospective)

### 成功之处
*   成功在 12GB 显存的消费级显卡上跑通了 SFT + GRPO 全流程。
*   模型在 BFCL 基准上的表现超出了预期，验证了小模型在特定任务上的潜力。

### 改进空间
*   目前仅支持单步工具调用，未来可以引入多步推理（Multi-step Reasoning）和反思机制。
*   监控系统可以进一步集成链路追踪（Tracing），以便更精细地分析每个请求的延迟分布。

---

## 16. 行业应用场景 (Industry Use Cases)

### 场景 1：智能家居控制
*   **应用**：在智能音箱或手机端运行 Qwen3-0.6B，通过 Function Calling 控制灯光、空调等设备。
*   **优势**：低延迟、隐私保护（数据不出本地）。

### 场景 2：企业内部助手
*   **应用**：集成企业内部 API（如请假审批、会议室预订），通过自然语言交互完成操作。
*   **优势**：降低员工使用内部系统的门槛，提升效率。

---

## 17. 开发者心得 (Developer Insights)

*   **心得 1**：小模型微调的关键在于数据的纯净度。在 Function Calling 任务中，一个错误的 JSON 样本可能会带偏整个模型的输出风格。
*   **心得 2**：强化学习不是万能的，它更像是一种“精修”。如果没有一个好的 SFT 模型作为起点，强化学习很难收敛。
*   **心得 3**：工程化部署和模型训练同样重要。一个无法稳定上线、无法监控性能的模型，在工业界是没有价值的。

---

## 18. 项目架构图 (Project Architecture)

```text
[用户输入] -> [vLLM 推理服务 (K8s)] -> [Qwen3-0.6B (SFT+GRPO)]
                                          |
                                          v
[工具调用 JSON] <- [Agent 执行引擎] <- [模型输出]
      |
      v
[外部 API / 工具] -> [执行结果] -> [模型总结回复] -> [用户]
```

---

## 19. 结语 (Conclusion)

本项目通过对 Qwen3-0.6B 的深度定制，不仅实现了高质量的 Function Calling 能力，更探索了一套完整的小模型工程化落地流程。在 2026 年 AI 浪潮中，这种“小而美”的专业化模型将成为企业降本增效、实现端侧智能的关键。

---

## 附录：核心术语表 (Glossary)

| 术语 | 全称 | 简要定义 |
| :--- | :--- | :--- |
| **LLM** | Large Language Model | 大语言模型 |
| **SFT** | Supervised Fine-Tuning | 监督微调 |
| **RLHF** | Reinforcement Learning from Human Feedback | 基于人类反馈的强化学习 |
| **GRPO** | Group Relative Policy Optimization | 组内相对策略优化 |
| **LoRA** | Low-Rank Adaptation | 低秩适配 |
| **KV Cache** | Key-Value Cache | 键值缓存，用于加速推理 |
| **TTFT** | Time To First Token | 首 token 延迟 |
| **TPS** | Tokens Per Second | 每秒生成的 token 数 |
| **HPA** | Horizontal Pod Autoscaler | 水平 Pod 自动扩缩容 |
| **MCP** | Model Context Protocol | 模型上下文协议 |
