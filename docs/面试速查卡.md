# 🚀 Qwen3-0.6B Function Calling 面试速查卡 (30分钟冲刺)

---

## 📑 速查卡 1: LoRA 微调 (Low-Rank Adaptation)
**核心原理**：冻结原模型权重 $W$，通过学习两个低秩矩阵 $A$ 和 $B$ 来近似权重更新 $\Delta W$。

### 🔹 核心公式
$$W_{new} = W + \Delta W = W + B \times A$$
*其中 $A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times d}$，秩 $r \ll d$。*

### 🔹 本项目关键参数
| 参数 | 设定值 | 面试话术 |
| :--- | :--- | :--- |
| **Rank (r)** | **32** | 平衡表达能力与显存，0.6B模型够用了 |
| **Alpha ($\alpha$)** | **64** | 缩放因子，通常设为 $2 \times r$，稳定训练 |
| **Target** | **all** | 覆盖所有线性层（q, v, k, o, up, down, gate）效果最好 |
| **Dropout** | **0.05** | 防止小模型在60k数据上过拟合 |

### 🔹 优劣势对比
- ✅ **优点**：显存占用极低（仅需存 $A, B$ 梯度）、训练快、可无损合并、支持多任务切换。
- ❌ **缺点**：理论上限略低于全量微调（Full FT），但在本项目 FC 任务中差距极小。

### 🔹 常见陷阱 & 避坑
- **Rank太小**：模型学不会复杂的 JSON Schema 约束。
- **Rank太大**：容易过拟合，且失去参数高效的优势。
- **忘记合并**：推理前需将 LoRA 权重 merge 回基座，否则推理延迟增加。

---

## 🧠 速查卡 2: GRPO 强化学习 (Group Relative Policy Optimization)
**核心原理**：DeepSeek-R1 同款算法。通过组内相对评分代替 Critic 模型，极大节省显存。

### 🔹 算法流程
1. **Group Sampling**：对同一 Prompt 生成 $G$ 个候选回答（本项目 $G=4$）。
2. **Reward Calc**：使用规则/模型对 $G$ 个回答打分。
3. **Advantage Normalization**：组内分数减去均值并除以标准差，得到相对优势。
4. **Policy Update**：根据优势值更新策略网络。

### 🔹 奖励函数设计 (Reward Formula)
**Total Reward = $0.25 \times$ Parse + $0.25 \times$ Schema + $0.25 \times$ Exec + $0.25 \times$ Semantic**
- **Parse**: 是否为合法 JSON。
- **Schema**: 是否符合函数定义的参数类型/必填项。
- **Exec**: 模拟执行是否报错。
- **Semantic**: 参数值与 Ground Truth 的 F1 相似度。

### 🔹 方案对比
| 特性 | **GRPO** | PPO | DPO |
| :--- | :--- | :--- | :--- |
| **额外模型** | **无 (Ref模型可显存共享)** | 需要 Critic 和 Reward 模型 | 无 |
| **数据要求** | **Prompt + 规则奖励** | 需要偏好排序对 | 需要偏好排序对 |
| **显存占用** | **极低 (适合12GB)** | 极高 | 中等 |

---

## ⚡ 速查卡 3: vLLM 推理优化
**核心原理**：通过 PagedAttention 解决显存碎片化，实现高吞吐推理。

### 🔹 PagedAttention
- **机制**：模仿操作系统虚拟内存，将 KV Cache 存储在不连续的物理块中。
- **块大小**：本项目设定 **16 tokens/block**。
- **收益**：显存利用率提升至 90%+，支持更大 Batch Size。

### 🔹 关键性能指标 (Metrics)
- **TTFT (Time-To-First-Token)**：首 token 延迟，影响用户感知。
- **Throughput (吞吐量)**：每秒处理的 token 总数。
- **TBT (Time-Between-Tokens)**：token 间的平均延迟。

### 🔹 进阶特性
- **Continuous Batching**：动态加入/退出请求，不需等待整个 Batch 完成。
- **Prefix Caching**：多个请求共享相同 System Prompt 时，缓存只存一份。

### 🔹 本项目实测 (12GB GPU / Qwen3-0.6B)
- **显存占用**：推理时约占 2-4GB（含 KV Cache 预分配）。
- **并发能力**：单卡支持 32+ 并发，吞吐量显著优于原生 Transformers。

---

## 📊 速查卡 4: 评估指标 (Evaluation)
**核心原理**：针对 Function Calling 任务的 6 维离线评测体系。

### 🔹 离线指标公式
1. **Parse Rate** = `parse_success / total` (JSON 格式正确)
2. **Schema Hit** = `schema_valid / total` (符合函数定义)
3. **Func Accuracy** = `name_match / total` (函数名对不对)
4. **Param F1** = $2 \times \frac{P \times R}{P + R}$ (参数键值对的精确率与召回率)
5. **Exec Rate** = `exec_success / total` (模拟执行通过率)

### 🔹 性能演进 (Target Numbers)
| 阶段 | Parse Rate | Schema Hit | Exec Rate |
| :--- | :--- | :--- | :--- |
| **Base (0.6B)** | ~5% | ~3% | ~1% |
| **SFT** | **95%+** | **88%+** | **80%+** |
| **SFT+GRPO** | **97%+** | **92%+** | **88%+** |

### 🔹 在线指标
- **TTFT P50/P95**：50%/95% 的请求首词延迟在 X ms 内。
- **Throughput**：生产环境下每秒处理的请求数。

---

## 🚢 速查卡 5: 部署运维 (MLOps)
**核心原理**：容器化封装 + K8s 编排 + 实时监控。

### 🔹 常用命令 (Cheat Sheet)
- **Docker**:
  - `docker build -t qwen-fc:v1 -f deploy/docker/Dockerfile.inference .`
  - `docker run --gpus all -p 8000:8000 qwen-fc:v1`
- **K8s**:
  - `kubectl apply -f deploy/k8s/deployment.yaml`
  - `kubectl logs -f deployment/qwen-fc-vllm`
- **Helm**:
  - `helm install qwen-fc ./deploy/helm`
  - `helm rollback qwen-fc 1`

### 🔹 监控 (Prometheus & Grafana)
- **PromQL 示例**:
  - 吞吐量：`rate(vllm:num_tokens_total[5m])`
  - 显存占用：`vllm:gpu_cache_usage_perc`
- **Grafana 核心面板**:
  1. **TTFT P95** (延迟监控)
  2. **Requests per Second** (负载监控)
  3. **GPU Memory Usage** (资源监控)

---
**💡 面试小贴士**：
- 被问到“为什么用0.6B”：强调**端侧部署**潜力与**低成本验证**。
- 被问到“GRPO优势”：强调**不需要Critic模型**，在12GB显存下实现了强化学习对齐。
- 被问到“FC难点”：提到 **JSON 幻觉** 和 **参数类型约束**，通过 GRPO 规则奖励解决。