# Qwen-0.6-ft 项目代码走读指南

本指南旨在帮助你在面试中流畅地讲解本项目核心代码。通过对 5 个关键文件的深入剖析，你将能够应对“请讲解一下这段代码”、“你是如何实现数据预处理/评测/奖励函数的”等典型面试问题。

---

## File 1: scripts/prepare_data.py

### 功能概述
该脚本负责从 HuggingFace 下载 `xlam-function-calling-60k` 数据集，并将其转换为 LLaMA-Factory 原生支持的 `sharegpt` 格式，最后按 90/10 比例划分训练集和验证集。

### 关键代码段

#### 段 1: 数据下载与加载
```python
# Lines 73-96
def load_dataset_from_hf():
    # ...
    dataset = load_dataset(DATASET_NAME, split="train")
    print(f"[✓] 数据集加载完成，共 {len(dataset)} 条样本")
    return dataset
```
**讲解要点**: 使用 HuggingFace `datasets` 库实现流式或缓存加载。提到 xlam-60k 是 gated 数据集，需要 `huggingface-cli login` 授权。
**面试关联**: “你是如何获取和处理原始数据的？”

#### 段 2: ShareGPT 格式转换逻辑
```python
# Lines 143-171
        # 构建对话序列
        conversations = [{"from": "human", "value": query}]

        # 为每个工具调用添加 function_call + observation 对
        for i, answer in enumerate(answers):
            function_call_obj = {
                "name": answer["name"],
                "arguments": answer.get("arguments", {}),
            }
            conversations.append({
                "from": "function_call",
                "value": json.dumps(function_call_obj, ensure_ascii=False),
            })
            conversations.append({
                "from": "observation",
                "value": json.dumps({"status": "success"}, ensure_ascii=False),
            })

        # 最后的 gpt 回复
        conversations.append({"from": "gpt", "value": gpt_response})
```
**讲解要点**: 重点解释 `function_call` 和 `observation` 角色的作用。LLaMA-Factory 要求严格的对话交替格式。通过模拟 `observation`（工具执行结果），让模型学会处理函数调用的完整闭环。
**面试关联**: “为什么选择 ShareGPT 格式？”、“如何处理多轮对话或多工具调用？”

#### 段 3: 训练/验证集划分
```python
# Lines 229-238
    random.seed(RANDOM_SEED)
    random.shuffle(converted_data)

    split_idx = int(len(converted_data) * TRAIN_RATIO)
    train_data = converted_data[:split_idx]
    val_data = converted_data[split_idx:]
```
**讲解要点**: 强调 `random.seed(42)` 的重要性（实验可复现性）。解释 90/10 划分是为了在保证训练数据充足的同时，有足够的验证集来监控过拟合。
**面试关联**: “你的训练集和验证集是如何划分的？为什么要这么分？”

### 面试官可能追问
- **Q**: 如果数据中包含非法 JSON 怎么办？
- **A**: 代码在 `convert_sample` 中使用了 `try...except` 块（Line 182），捕获 `JSONDecodeError` 并跳过该样本，保证了数据清洗的鲁棒性。

---

## File 2: scripts/merge_lora.py

### 功能概述
该脚本将训练得到的 LoRA 适配器（Adapter）权重合并回原始的 Qwen3-0.6B 基座模型，生成一个可以直接部署的独立模型。

### 关键代码段

#### 段 1: 加载模型与适配器
```python
# Lines 184-204
    base_model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map=args.device_map,
        trust_remote_code=True,
    )
    # ...
    model_with_lora = PeftModel.from_pretrained(
        base_model,
        args.adapter_path,
        torch_dtype=torch.bfloat16,
    )
```
**讲解要点**: 解释 `torch.bfloat16` 的使用（平衡精度与显存）。`PeftModel.from_pretrained` 会在基座模型上动态注入 LoRA 层。
**面试关联**: “合并 LoRA 时需要注意什么？”（显存、数据类型一致性）。

#### 段 2: 核心合并操作
```python
# Lines 213-220
    # merge: 将 LoRA 的 delta 权重加到原始权重上
    # unload: 移除 LoRA 模块，恢复模型的原始结构
    print("\n[4/5] 合并 LoRA 权重到基座模型...")
    merged_model = model_with_lora.merge_and_unload()
```
**讲解要点**: 深入讲解 `merge_and_unload()` 的数学原理：$W_{new} = W_{base} + \frac{\alpha}{r} (B \times A)$。合并后模型结构回归原始状态，不再依赖 PEFT 库。
**面试关联**: “LoRA 合并的原理是什么？”、“为什么要进行合并？”（减少推理延迟、简化部署）。

### 面试官可能追问
- **Q**: 合并过程中显存不足怎么办？
- **A**: 可以通过设置 `--device-map cpu`（Line 88-93）强制在 CPU 上进行合并，虽然速度慢，但可以处理显存受限的情况。

---

## File 3: eval/metrics.py

### 功能概述
该模块实现了 6 个核心评测指标，从格式、意图、参数、可执行性等多个维度全面评估模型在函数调用任务上的表现。

### 关键代码段

#### 段 1: 鲁棒的 JSON 解析
```python
# Lines 75-89
    # 有时候模型会在 JSON 前后加一些文字，尝试提取 JSON 部分
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        try:
            parsed = json.loads(text[start : end + 1])
            # ...
```
**讲解要点**: 解释为什么不能直接 `json.loads`。模型可能会输出“Here is the call: {...}”，通过寻找首尾花括号可以大幅提高解析成功率（Parse Rate）。
**面试关联**: “如何处理模型输出中的杂质文字？”

#### 段 2: 参数级 F1 计算
```python
# Lines 215-230
        pred_pairs = {
            (k, json.dumps(v, sort_keys=True, ensure_ascii=False))
            for k, v in pred_args.items()
        }
        ref_pairs = {
            (k, json.dumps(v, sort_keys=True, ensure_ascii=False))
            for k, v in ref_args.items()
        }
        # 交集 = 预测正确的参数
        correct = pred_pairs & ref_pairs
```
**讲解要点**: 解释为什么用 F1 而不是 Accuracy。将参数转化为 `(key, value)` 集合，利用集合交集快速计算 True Positives。使用 `json.dumps` 统一不同数据类型的比较标准。
**面试关联**: “如何精确评估模型生成的参数准确性？”

#### 段 3: Schema 命中与可执行性
```python
# Lines 305-311 (Schema Hit) & 386-395 (Exec Rate)
        # 检查必填参数是否齐全
        all_required_present = all(r in pred_args for r in required)
        
        # 检查参数类型是否正确
        if not isinstance(param_value, expected_type):
            type_ok = False
```
**讲解要点**: `schema_hit_rate` 关注“必填项是否漏填”，`exec_rate` 关注“类型是否填错”。这两个指标模拟了真实 API 调用的前置校验。
**面试关联**: “什么是 Schema 命中率？它和函数名准确率有什么区别？”

### 面试官可能追问
- **Q**: 为什么需要 `exec_rate`？
- **A**: 因为即使函数名和参数名都对，如果类型错误（如 `age` 传了字符串），API 依然会报错。`exec_rate` 是衡量模型输出“生产就绪度”的关键指标。

---

## File 4: scripts/grpo_reward.py

### 功能概述
该脚本定义了 GRPO 强化学习训练所需的规则奖励函数，包含 4 个维度，通过组内相对比较来优化模型。

### 关键代码段

#### 段 1: 奖励函数组合
```python
# Lines 442-469
def combined_reward(
    # ...
    # 默认权重（各 0.25）
    default_weights = {
        "json_parse": 0.25,
        "schema_hit": 0.25,
        "exec": 0.25,
        "semantic": 0.25,
    }
```
**讲解要点**: 解释 GRPO 的核心思想：不需要奖励模型（RM），直接用规则打分。这 4 个维度分别对应：格式门槛、意图对齐、编程正确性、语义准确性。
**面试关联**: “你是如何设计强化学习的奖励函数的？”、“为什么给这四个维度分配相等的权重？”

#### 段 2: 语义匹配奖励 (Semantic Reward)
```python
# Lines 422-435
    tp = sum(
        1 for key in pred_args
        if key in gt_args and values_match(pred_args[key], gt_args[key])
    )
    precision = tp / len(pred_args) if pred_args else 0.0
    recall = tp / len(gt_args) if gt_args else 0.0
    f1 = 2 * precision * recall / (precision + recall)
```
**讲解要点**: 这里的奖励基于 F1 分数，是一个连续值（0.0-1.0），比 0/1 奖励更能提供细腻的梯度信号，帮助模型在强化学习中逐步改进参数填充。
**面试关联**: “在强化学习中，稀疏奖励和密集奖励你更倾向于哪种？”

### 面试官可能追问
- **Q**: GRPO 相比 PPO 的优势在哪里？
- **A**: GRPO 舍弃了复杂的 Value Network，通过组内样本的相对奖励来估计优势函数（Advantage），极大地降低了显存开销和训练复杂度，非常适合 Function Calling 这种规则明确的任务。

---

## File 5: configs/qwen3_lora_sft.yaml

### 功能概述
这是 LLaMA-Factory 的训练配置文件，定义了模型路径、LoRA 超参数、训练策略以及针对 12GB 显存的优化设置。

### 关键代码段

#### 段 1: LoRA 配置
```yaml
# Lines 50-70
finetuning_type: lora
lora_target: all
lora_rank: 32
lora_alpha: 64
```
**讲解要点**: 解释为什么 `lora_target` 选 `all`（覆盖所有线性层效果更好）。解释 `rank=32` 和 `alpha=64` 的选择（经验法则：alpha 通常为 rank 的 2 倍）。
**面试关联**: “你是如何调整 LoRA 参数的？”

#### 段 2: 显存优化策略
```yaml
# Lines 105-113 & 189
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
gradient_checkpointing: true
```
**讲解要点**: 这是针对 12GB 显存的核心配置。通过小 `batch_size` + 大 `gradient_accumulation` 模拟大 batch 效果（等效 batch size = 16）。开启 `gradient_checkpointing` 以时间换空间，防止 OOM。
**面试关联**: “在显存受限的情况下，你采取了哪些优化措施？”

### 面试官可能追问
- **Q**: 为什么学习率设置在 `2e-4`？
- **A**: LoRA 训练的学习率通常比全参数微调（~1e-5）要大一个数量级，因为 LoRA 只更新极少量的参数，需要更大的步长来加速收敛。

---

## 总结：面试讲解技巧
1. **先总后分**: 先说文件是干什么的，再说关键函数。
2. **联系实际**: 解释代码时，多提“为了防止 OOM”、“为了处理模型幻觉”等实际开发中的考量。
3. **展示深度**: 提到 `bfloat16`、`GRPO 优势函数`、`LoRA 数学原理`等底层知识。
4. **关注边界**: 主动提到代码中对异常（如 JSON 解析失败）的处理，展示工程严谨性。
