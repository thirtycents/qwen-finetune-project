#!/usr/bin/env python3
"""
============================================================
evaluate.py - ç¦»çº¿è¯„æµ‹ä¸»è„šæœ¬
============================================================
åŠŸèƒ½ï¼šè¯»å–æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”ï¼Œè®¡ç®—å„é¡¹è¯„æµ‹æŒ‡æ ‡ã€‚

èƒŒæ™¯çŸ¥è¯†ï¼š
-----------
"ç¦»çº¿è¯„æµ‹"æ˜¯æŒ‡åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œç”¨ä¸€ç»„æµ‹è¯•æ•°æ®æ¥è¯„ä¼°æ¨¡å‹çš„è´¨é‡ã€‚
ä¸"åœ¨çº¿è¯„æµ‹"ï¼ˆå®æ—¶æµ‹è¯•æœåŠ¡æ€§èƒ½ï¼‰ä¸åŒï¼Œç¦»çº¿è¯„æµ‹å…³æ³¨çš„æ˜¯ï¼š
- æ¨¡å‹çš„è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ
- è°ƒç”¨çš„å‡½æ•°æ˜¯å¦æ­£ç¡®ï¼Ÿ
- å‚æ•°æ˜¯å¦å¡«å¯¹äº†ï¼Ÿ

ä½¿ç”¨æµç¨‹ï¼š
-----------
1. å…ˆç”¨ run_inference.py ç”Ÿæˆé¢„æµ‹ç»“æœï¼ˆä¿å­˜ä¸º JSONL æ–‡ä»¶ï¼‰
2. è¿è¡Œæœ¬è„šæœ¬è®¡ç®—æŒ‡æ ‡

ä½¿ç”¨æ–¹å¼ï¼š
-----------
    python eval/evaluate.py \\
        --predictions eval/predictions.jsonl \\
        --ground-truth data/processed/val.json \\
        --output eval/results.json

è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼š
-----------
predictions.jsonlï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰ï¼š
    {"prediction": "æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬", "index": 0}
    {"prediction": "æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬", "index": 1}

val.jsonï¼ˆLLaMA-Factory sharegpt æ ¼å¼çš„æ•°ç»„ï¼‰ï¼š
    [{"conversations": [...], "system": "...", "tools": "..."}, ...]
============================================================
"""

import argparse
import json
import sys
from pathlib import Path

# å¯¼å…¥æˆ‘ä»¬çš„æŒ‡æ ‡è®¡ç®—æ¨¡å—
from eval.metrics import compute_all_metrics, parse_function_call


def load_predictions(filepath: str) -> list[str]:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœã€‚

    JSONL = JSON Linesï¼Œæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ã€‚
    è¿™ç§æ ¼å¼é€‚åˆå¤§é‡æ•°æ®ï¼Œå› ä¸ºä¸éœ€è¦ä¸€æ¬¡æ€§è¯»å…¥æ•´ä¸ªæ•°ç»„ã€‚

    Args:
        filepath: JSONL æ–‡ä»¶è·¯å¾„

    Returns:
        é¢„æµ‹æ–‡æœ¬åˆ—è¡¨
    """
    predictions = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                predictions.append(obj.get("prediction", ""))
            except json.JSONDecodeError:
                predictions.append("")
    return predictions


def load_ground_truth(filepath: str) -> tuple[list[dict], list[list[dict]]]:
    """
    ä»éªŒè¯é›†æ–‡ä»¶åŠ è½½æ ‡å‡†ç­”æ¡ˆå’Œå·¥å…·åˆ—è¡¨ã€‚

    è§£æ sharegpt æ ¼å¼çš„æ•°æ®ï¼Œæå–ï¼š
    1. æ ‡å‡†ç­”æ¡ˆï¼ˆfunction_call è§’è‰²çš„å†…å®¹ï¼‰
    2. å¯ç”¨å·¥å…·åˆ—è¡¨

    Args:
        filepath: éªŒè¯é›† JSON æ–‡ä»¶è·¯å¾„

    Returns:
        (references, tools_list)
        - references: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ {"name": ..., "arguments": ...}
        - tools_list: æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„å·¥å…·åˆ—è¡¨
    """
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)

    references = []
    tools_list = []

    for sample in data:
        conversations = sample.get("conversations", [])
        tools_str = sample.get("tools", "[]")

        # è§£æå·¥å…·åˆ—è¡¨
        try:
            tools = json.loads(tools_str) if isinstance(tools_str, str) else tools_str
        except json.JSONDecodeError:
            tools = []
        tools_list.append(tools)

        # ä»å¯¹è¯ä¸­æå–ç¬¬ä¸€ä¸ª function_call ä½œä¸ºæ ‡å‡†ç­”æ¡ˆ
        ref = {"name": "", "arguments": {}}
        for conv in conversations:
            if conv.get("from") == "function_call":
                parsed = parse_function_call(conv.get("value", ""))
                if parsed is not None:
                    ref = parsed
                break  # åªå–ç¬¬ä¸€ä¸ª function_call

        references.append(ref)

    return references, tools_list


def print_results(results: dict) -> None:
    """
    æ ¼å¼åŒ–æ‰“å°è¯„æµ‹ç»“æœã€‚

    ç”¨è¡¨æ ¼å½¢å¼å±•ç¤ºå„é¡¹æŒ‡æ ‡ï¼Œæ–¹ä¾¿é˜…è¯»ã€‚
    """
    print()
    print("=" * 60)
    print("  ç¦»çº¿è¯„æµ‹ç»“æœ")
    print("=" * 60)
    print()
    print(f"  {'æŒ‡æ ‡':<25} {'å€¼':>10}")
    print(f"  {'-'*25} {'-'*10}")

    # æŒ‡æ ‡åç§°æ˜ å°„ï¼ˆè‹±æ–‡â†’ä¸­æ–‡ï¼‰
    metric_names = {
        "parse_rate": "è§£ææˆåŠŸç‡ (Parse Rate)",
        "func_name_accuracy": "å‡½æ•°åå‡†ç¡®ç‡ (Name Acc)",
        "param_precision": "å‚æ•°ç²¾ç¡®ç‡ (Precision)",
        "param_recall": "å‚æ•°å¬å›ç‡ (Recall)",
        "param_f1": "å‚æ•° F1 å€¼ (F1)",
        "schema_hit_rate": "Schema å‘½ä¸­ç‡",
        "exec_rate": "å¯æ‰§è¡Œç‡ (Exec Rate)",
    }

    for key, name in metric_names.items():
        value = results.get(key)
        if value is None:
            print(f"  {name:<25} {'N/A':>10}")
        else:
            print(f"  {name:<25} {value:>10.4f}")

    print()
    print("=" * 60)

    # ç®€è¦è§£è¯»
    parse_r = results.get("parse_rate", 0)
    if parse_r >= 0.95:
        print("  ğŸ“Š è§£æç‡ â‰¥ 95%ï¼Œæ¨¡å‹å·²å¾ˆå¥½åœ°å­¦ä¼šäº† JSON è¾“å‡ºæ ¼å¼")
    elif parse_r >= 0.80:
        print("  ğŸ“Š è§£æç‡åœ¨ 80-95%ï¼Œæ ¼å¼å­¦ä¹ åŸºæœ¬åˆ°ä½ï¼Œä»æœ‰æ”¹è¿›ç©ºé—´")
    else:
        print("  ğŸ“Š è§£æç‡ < 80%ï¼Œæ¨¡å‹å°šæœªå……åˆ†å­¦ä¼šæ­£ç¡®æ ¼å¼ï¼Œå»ºè®®å¢åŠ è®­ç»ƒ")

    print()


def main():
    """ä¸»å‡½æ•°"""

    parser = argparse.ArgumentParser(
        description="è¯„æµ‹å‡½æ•°è°ƒç”¨æ¨¡å‹çš„ç¦»çº¿è´¨é‡æŒ‡æ ‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python eval/evaluate.py --predictions eval/predictions.jsonl
  python eval/evaluate.py --predictions eval/predictions.jsonl --ground-truth data/processed/val.json
        """,
    )
    parser.add_argument(
        "--predictions",
        type=str,
        required=True,
        help="æ¨¡å‹é¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰",
    )
    parser.add_argument(
        "--ground-truth",
        type=str,
        default="data/processed/val.json",
        help="æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/processed/val.jsonï¼‰",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="eval/results.json",
        help="è¯„æµ‹ç»“æœè¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤: eval/results.jsonï¼‰",
    )

    args = parser.parse_args()

    # Step 1: æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.predictions).exists():
        print(f"[é”™è¯¯] é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {args.predictions}")
        print("è¯·å…ˆè¿è¡Œ: python eval/run_inference.py")
        sys.exit(1)

    if not Path(args.ground_truth).exists():
        print(f"[é”™è¯¯] æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶ä¸å­˜åœ¨: {args.ground_truth}")
        print("è¯·å…ˆè¿è¡Œ: python scripts/prepare_data.py")
        sys.exit(1)

    # Step 2: åŠ è½½æ•°æ®
    print("[*] åŠ è½½é¢„æµ‹ç»“æœ...")
    predictions = load_predictions(args.predictions)
    print(f"    å…± {len(predictions)} æ¡é¢„æµ‹")

    print("[*] åŠ è½½æ ‡å‡†ç­”æ¡ˆ...")
    references, tools_list = load_ground_truth(args.ground_truth)
    print(f"    å…± {len(references)} æ¡æ ‡å‡†ç­”æ¡ˆ")

    # æ•°é‡å¯¹é½ï¼ˆå–è¾ƒçŸ­çš„ï¼‰
    n = min(len(predictions), len(references))
    if len(predictions) != len(references):
        print(f"    [è­¦å‘Š] é¢„æµ‹æ•°é‡({len(predictions)}) â‰  æ ‡å‡†ç­”æ¡ˆæ•°é‡({len(references)})ï¼Œå–å‰ {n} æ¡")
    predictions = predictions[:n]
    references = references[:n]
    tools_list = tools_list[:n]

    # Step 3: è®¡ç®—æŒ‡æ ‡
    print("[*] è®¡ç®—è¯„æµ‹æŒ‡æ ‡...")
    results = compute_all_metrics(predictions, references, tools_list)

    # Step 4: æ‰“å°ç»“æœ
    print_results(results)

    # Step 5: ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"[âœ“] è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print()


if __name__ == "__main__":
    main()
