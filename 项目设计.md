# Function Calling 小模型工程化项目（LLaMA-Factory + vLLM Production Stack）

## 1. 项目一句话

用 Qwen3-0.6B 做“函数/工具调用助手（Function Calling）”的后训练与生产化部署：SFT → GRPO 强化学习 → 导出 → vLLM K8s 部署 → 压测与监控 →（可选）端侧离线 GGUF Demo。

选择 Function Calling 的原因：数据集与公开基准齐全，且非常适合“小模型 + 强格式约束”的定位（可用严格可验证的指标评测）。

## 2. 技术栈与仓库选型

训练：LLaMA-Factory（统一微调/对齐训练入口，支持 SFT、DPO 等方法，且覆盖 Qwen3 等模型）([GitHub](https://github.com/hiyouga/LlamaFactory?utm_source=chatgpt.com "hiyouga/LlamaFactory: Unified Efficient Fine-Tuning of 100 ..."))  
服务：vLLM Production Stack（vLLM 官方参考生产部署栈，K8s-native，可扩展并配套监控/路由等）([GitHub](https://github.com/vllm-project/production-stack?utm_source=chatgpt.com "vllm-project/production-stack"))

对齐算法：GRPO 思路参考公开论文/描述（不依赖人工偏好对时更顺手）。([arXiv](https://arxiv.org/pdf/2402.03300?utm_source=chatgpt.com "DeepSeekMath"))

## 3. 目标任务定义（单点收敛）

输入：用户自然语言 + tools schema（JSON Schema 描述函数名、参数、类型、必填项）  
输出：严格格式的 tool call（例如 OpenAI function calling 风格的 JSON，或你选定的标签封装格式），必须能 parse 且满足 schema；必要时支持多函数调用/多轮。

关键约束（决定评估方式与 reward 设计）：  
1）可解析（JSON parse / AST parse）  
2）schema 命中（字段齐全、类型正确、枚举值合法）  
3）可执行（能真实调用 stub 函数或模拟执行）  
4）语义正确（参数值与用户意图一致）

## 4. 可直接用的训练/评测资源（尽量不自造数据）

训练数据（推荐主线）：Salesforce/xlam-function-calling-60k  
该数据集由 APIGen 生成，并强调三层校验：格式检查、真实函数执行、语义验证。([Hugging Face](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k?utm_source=chatgpt.com "Salesforce/xlam-function-calling-60k · Datasets at ..."))  
注意：数据集在 Hugging Face 上是 gated，需要点确认条款才能下载文件。([Hugging Face](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k/tree/main?utm_source=chatgpt.com "Salesforce/xlam-function-calling-60k at main"))

训练数据（备选）：NousResearch/hermes-function-calling-v1  
包含 function-calling / structured output 等样本，适合补充多样性。([Hugging Face](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1?utm_source=chatgpt.com "NousResearch/hermes-function-calling-v1 · Datasets at ..."))

评测基准（推荐主线）：Berkeley Function Calling Leaderboard（BFCL）  
BFCL 用于评估工具调用能力（含真实世界函数场景、可执行评估等思路），有数据与代码入口。([Gorilla](https://gorilla.cs.berkeley.edu/leaderboard.html?utm_source=chatgpt.com "Berkeley Function Calling Leaderboard (BFCL) V4 - Gorilla LLM"))

## 5. 训练与对齐路线（单卡友好）

你最终交付建议做 3 个模型版本，形成清晰对比：

A. Base：原始 Qwen3-0.6B（不训练）  
B. SFT：用 xLAM 60k 做 LoRA SFT（主版本）  
C. SFT+对齐（必须）：

- 优先建议：GRPO（用“可验证 reward”替代人工偏好对）([arXiv](https://arxiv.org/pdf/2402.03300?utm_source=chatgpt.com "DeepSeekMath"))
    
- 备选：DPO（如果你后续找到合适的 chosen/rejected 格式偏好数据再上）
    

为什么这里 GRPO 更顺：你可以把 reward 设计成自动可验证项（不需要人工偏好对），例如：  
1）JSON 是否可 parse（0/1）  
2）是否满足 schema（0/1 或分项得分）  
3）是否能执行（stub 调用是否成功，0/1）  
4）执行结果是否匹配预期（0/1 或基于规则的相似度）

## 6. 评估与指标（离线 + 在线）

离线（质量）：  
1）严格格式匹配率：parse 成功率、schema 命中率  
2）函数名准确率：Top-1 命中率  
3）参数级指标：字段级准确率/召回/F1（每个字段是否正确）  
4）可执行率：stub 执行成功率  
5）BFCL 跑分：复现 BFCL 的评测脚本流程（当作“公开基准对齐”）。([Gorilla](https://gorilla.cs.berkeley.edu/leaderboard.html?utm_source=chatgpt.com "Berkeley Function Calling Leaderboard (BFCL) V4 - Gorilla LLM"))

在线（服务性能，vLLM）：

- TTFT（Time-To-First-Token）
    
- tokens/s（吞吐，Input/Output TPS）
    
- P50/P95（TTFT、E2E latency、TPOT 等）
    

vLLM 文档里有 TTFT 等指标定义与 metrics 端点说明，可直接接 Prometheus/Grafana。([vLLM](https://docs.vllm.ai/en/latest/design/metrics/?utm_source=chatgpt.com "Metrics - vLLM"))

## 7. vLLM 调参与压测矩阵（写进 README 的“工程亮点”）

固定模型版本（例如 SFT 最佳权重），做压测矩阵并输出表格/曲线：

维度建议：  
1）并发：{1, 2, 4, 8, 16, 32}  
2）上下文长度（prompt tokens）：{256, 1k, 2k, 4k}（按你显存能力）  
3）vLLM 关键参数：max_num_seqs、max_num_batched_tokens（按官方建议范围探索）

输出：

- 每个并发下的 TTFT P50/P95
    
- tokens/s（总吞吐）
    
- GPU 利用率/显存峰值（可选）
    
- 结论：给出“推荐线上默认参数组合”
    

## 8. 工程交付（你要交付的“像样工程”清单）

1）Docker 镜像：训练镜像（可选）+ 推理镜像（必须）  
2）K8s：Deployment/Service/Ingress 或 Helm chart（推荐直接基于 production-stack）([GitHub](https://github.com/vllm-project/production-stack?utm_source=chatgpt.com "vllm-project/production-stack"))  
3）监控：/metrics 接 Prometheus + Grafana dashboard（production-stack 也把监控作为目标之一）([GitHub](https://github.com/vllm-project/production-stack?utm_source=chatgpt.com "vllm-project/production-stack"))  
4）CI/CD：

- 训练侧：lint + 单元测试（reward/解析器）+ 产物上传（可选）
    
- 服务侧：build 镜像 → push → 部署（GitHub Actions/GitLab CI）
    

## 9. “一条命令”可复现（强烈建议）

你 README 的目标体验是三条命令：

1）一条命令训练（SFT + GRPO 全流程）  
2）一条命令起服务（vLLM OpenAI-compatible API，走 production-stack）([vLLM](https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/?utm_source=chatgpt.com "Production stack - vLLM"))  
3）一条命令压测（输出 TTFT/TPS/P95 报表）

## 10.双部署形态：云端在线 + 端侧离线

A. 云端在线（主线不变）

- vLLM production-stack 上 K8s 部署，展示吞吐/延迟/弹性/监控。([vLLM](https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/?utm_source=chatgpt.com "Production stack - vLLM"))
    

B. 端侧离线 Demo（加分项）

- 训练完合并 LoRA → 导出 HF → 转 GGUF → 4bit/8bit 量化 → llama.cpp/Ollama 离线运行
    
- 端侧指标不要和 vLLM 拼吞吐，改成：包体大小、内存占用、首 token 延迟、离线可用/隐私等
    

## 11. 你最终要交付的产物列表（面试官最关心）

1）代码：训练脚本/配置、reward 解析器、评测脚本、服务部署 chart、压测工具  
2）模型：Base / SFT / SFT+GRPO 的权重或 adapter  
3）结果：一张表 + 两张曲线

- 表：离线质量（parse/schema/exec/F1/BFCL 可选）
    
- 曲线1：SFT vs SFT+对齐 的离线指标提升
    
- 曲线2：并发压测下 TTFT P95 与 tokens/s  
    4）README：三条命令复现 + 架构图 + 参数选择结论
    
